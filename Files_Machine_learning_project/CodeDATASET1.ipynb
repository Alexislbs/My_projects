{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\incen\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\incen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\incen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#IMPORTATIONS GENERALES\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "#import matplotlib.pyPhrase as plt\n",
    "import pickle\n",
    "#SKLEARN \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "   #LES 4 MODELES\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "#AUTRE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FONCTIONS ET DICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_comma(file,newfile):\n",
    "    data_frame = pd.read_csv(file)\n",
    "\n",
    "    # Retirer les virgules sauf la premi√®re de chaque ligne\n",
    "    data_frame.iloc[:, 1:] =  data_frame.iloc[:, 1:].replace(to_replace=',', value='', regex=True)\n",
    "# Enregistrer dans un nouveau fichier CSV avec un autre nom\n",
    "    nom_fichier_sortie = newfile\n",
    "    data_frame.to_csv(nom_fichier_sortie, index=False)\n",
    "    return data_frame\n",
    "\n",
    "    \n",
    "\n",
    "models = {\n",
    "    'SVM': SVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Perceptron': Perceptron(class_weight='balanced')\n",
    "}\n",
    "\n",
    "def printresult(model_name,accuracy,classification_report_result):\n",
    "    print(f'\\nResults for {model_name}:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('Classification Report:')\n",
    "    print(classification_report_result)\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "def evaluation(predictions,y_test):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    classification_report_result = classification_report(y_test, predictions)\n",
    "    return accuracy, classification_report_result\n",
    "\n",
    "def lemmatiser(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREMIERE MISE EN FORME DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7433\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#dataiemocap= dataiemocap.sample(n=1000, random_state=7)\n",
    "\n",
    "data_path = 'IEMOCAP_features.pkl'\n",
    "\n",
    "# users should use this instructor to load pkl dataset. \n",
    "videoIDs, videoSpeakers, videoLabels, videoText,\\\n",
    "    videoAudio, videoVisual, videoSentence, trainVid,\\\n",
    "        testVid = pickle.load(open(data_path, 'rb'), encoding='latin1')\n",
    "#IEMOCAP \n",
    "#ON MET DANS UN CSV\n",
    "\n",
    "data_list = []\n",
    "for video_id, phrases in videoSentence.items():\n",
    "    emotions = videoLabels.get(video_id, [])  \n",
    "    for emotion, phrase in zip(emotions, phrases):\n",
    "        data_list.append({'Emotion': emotion, 'Phrase': phrase})\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv('output.csv', index=False)\n",
    "\n",
    "dataiemocap = preprocess_comma(\"output.csv\",\"dataiemocap.csv\")\n",
    "\n",
    "print(len(dataiemocap))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "[\"You're\", 'not', 'sorry', 'you', 'came?']\n"
     ]
    }
   ],
   "source": [
    "leng = dataiemocap['Phrase'][0]\n",
    "leng = leng.split()\n",
    "print(len(leng))\n",
    "print(leng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7433\n",
      "5203\n",
      "Training SVM...\n",
      "-----------------COUNTVECTORIZER------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.2789237668161435\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.11      0.17       178\n",
      "           1       0.50      0.16      0.24       315\n",
      "           2       0.23      0.88      0.37       509\n",
      "           3       0.68      0.10      0.17       342\n",
      "           4       0.43      0.04      0.07       337\n",
      "           5       0.67      0.10      0.18       549\n",
      "\n",
      "    accuracy                           0.28      2230\n",
      "   macro avg       0.48      0.23      0.20      2230\n",
      "weighted avg       0.49      0.28      0.21      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "---------------------TFIDF------------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5417040358744395\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.40      0.39       178\n",
      "           1       0.54      0.58      0.56       315\n",
      "           2       0.50      0.52      0.51       509\n",
      "           3       0.64      0.59      0.61       342\n",
      "           4       0.62      0.58      0.60       337\n",
      "           5       0.53      0.53      0.53       549\n",
      "\n",
      "    accuracy                           0.54      2230\n",
      "   macro avg       0.54      0.53      0.53      2230\n",
      "weighted avg       0.54      0.54      0.54      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest...\n",
      "-----------------COUNTVECTORIZER------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.4905829596412556\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.31      0.33       178\n",
      "           1       0.51      0.60      0.55       315\n",
      "           2       0.47      0.44      0.46       509\n",
      "           3       0.55      0.51      0.53       342\n",
      "           4       0.57      0.39      0.46       337\n",
      "           5       0.47      0.58      0.52       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.49      0.47      0.48      2230\n",
      "weighted avg       0.49      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "---------------------TFIDF------------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5147982062780269\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.32      0.37       178\n",
      "           1       0.56      0.52      0.54       315\n",
      "           2       0.47      0.48      0.47       509\n",
      "           3       0.63      0.53      0.58       342\n",
      "           4       0.57      0.47      0.52       337\n",
      "           5       0.48      0.63      0.54       549\n",
      "\n",
      "    accuracy                           0.51      2230\n",
      "   macro avg       0.52      0.49      0.50      2230\n",
      "weighted avg       0.52      0.51      0.51      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes...\n",
      "-----------------COUNTVECTORIZER------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4600896860986547\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.07      0.12       178\n",
      "           1       0.69      0.36      0.47       315\n",
      "           2       0.43      0.50      0.46       509\n",
      "           3       0.72      0.34      0.46       342\n",
      "           4       0.64      0.31      0.42       337\n",
      "           5       0.38      0.78      0.51       549\n",
      "\n",
      "    accuracy                           0.46      2230\n",
      "   macro avg       0.56      0.39      0.41      2230\n",
      "weighted avg       0.53      0.46      0.44      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "---------------------TFIDF------------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4659192825112108\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.08      0.14       178\n",
      "           1       0.76      0.32      0.45       315\n",
      "           2       0.42      0.53      0.47       509\n",
      "           3       0.76      0.33      0.46       342\n",
      "           4       0.71      0.33      0.45       337\n",
      "           5       0.38      0.78      0.51       549\n",
      "\n",
      "    accuracy                           0.47      2230\n",
      "   macro avg       0.60      0.40      0.41      2230\n",
      "weighted avg       0.56      0.47      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron...\n",
      "-----------------COUNTVECTORIZER------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.38026905829596414\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.39      0.32       178\n",
      "           1       0.46      0.45      0.46       315\n",
      "           2       0.40      0.32      0.36       509\n",
      "           3       0.35      0.63      0.45       342\n",
      "           4       0.41      0.35      0.38       337\n",
      "           5       0.38      0.26      0.31       549\n",
      "\n",
      "    accuracy                           0.38      2230\n",
      "   macro avg       0.38      0.40      0.38      2230\n",
      "weighted avg       0.39      0.38      0.37      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "---------------------TFIDF------------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.42511210762331836\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.39      0.35       178\n",
      "           1       0.47      0.49      0.48       315\n",
      "           2       0.40      0.40      0.40       509\n",
      "           3       0.48      0.53      0.50       342\n",
      "           4       0.45      0.43      0.44       337\n",
      "           5       0.41      0.35      0.38       549\n",
      "\n",
      "    accuracy                           0.43      2230\n",
      "   macro avg       0.42      0.43      0.43      2230\n",
      "weighted avg       0.43      0.43      0.42      2230\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "X = dataiemocap['Phrase']\n",
    "y = dataiemocap['Emotion']\n",
    "print(len(X))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "print(len(X_train))\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    pipeline1 = make_pipeline(CountVectorizer(), model)\n",
    "    pipeline2 = make_pipeline(TfidfVectorizer(), model)\n",
    "# Entra√Æner le mod√®le\n",
    "    pipeline1.fit(X_train, y_train)\n",
    "    pipeline2.fit(X_train, y_train)\n",
    "# Pr√©dictions sur l'ensemble de test\n",
    "    predictions1 = pipeline1.predict(X_test)\n",
    "    predictions2 = pipeline2.predict(X_test)\n",
    "    \n",
    "# √âvaluation des performances du mod√®le avec COUNTVECTORIZER \n",
    "    accuracy1 , classification_report_result1 = evaluation(predictions1,y_test)\n",
    "    accuracy2 , classification_report_result2 = evaluation(predictions2,y_test)\n",
    "    # Affichage\n",
    "    print(\"-----------------COUNTVECTORIZER------------------\")\n",
    "    printresult(model_name,accuracy1,classification_report_result1)\n",
    "    print(\"---------------------TFIDF------------------------\")\n",
    "    printresult(model_name,accuracy2,classification_report_result2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON VA SURECHANTILLONNER LES CLASSES SOUS REPRESENTEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.44484304932735425\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.37      0.28       178\n",
      "           1       0.51      0.49      0.50       315\n",
      "           2       0.51      0.40      0.45       509\n",
      "           3       0.50      0.47      0.49       342\n",
      "           4       0.46      0.35      0.40       337\n",
      "           5       0.44      0.52      0.48       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.44      0.43      0.43      2230\n",
      "weighted avg       0.46      0.44      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.48654708520179374\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.36      0.33       178\n",
      "           1       0.50      0.57      0.53       315\n",
      "           2       0.51      0.42      0.46       509\n",
      "           3       0.51      0.56      0.53       342\n",
      "           4       0.48      0.49      0.49       337\n",
      "           5       0.52      0.50      0.51       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.47      0.48      0.47      2230\n",
      "weighted avg       0.49      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4838565022421525\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.35      0.30       178\n",
      "           1       0.47      0.51      0.49       315\n",
      "           2       0.52      0.42      0.46       509\n",
      "           3       0.52      0.52      0.52       342\n",
      "           4       0.55      0.52      0.53       337\n",
      "           5       0.49      0.53      0.51       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.47      0.47      0.47      2230\n",
      "weighted avg       0.49      0.48      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.44170403587443946\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.37      0.29       178\n",
      "           1       0.48      0.47      0.47       315\n",
      "           2       0.47      0.34      0.39       509\n",
      "           3       0.49      0.56      0.52       342\n",
      "           4       0.46      0.53      0.49       337\n",
      "           5       0.46      0.42      0.44       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.43      0.45      0.43      2230\n",
      "weighted avg       0.45      0.44      0.44      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5448430493273543\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.35      0.38       178\n",
      "           1       0.58      0.57      0.58       315\n",
      "           2       0.50      0.56      0.53       509\n",
      "           3       0.67      0.54      0.60       342\n",
      "           4       0.66      0.50      0.57       337\n",
      "           5       0.50      0.60      0.55       549\n",
      "\n",
      "    accuracy                           0.54      2230\n",
      "   macro avg       0.55      0.52      0.53      2230\n",
      "weighted avg       0.55      0.54      0.55      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.35      0.33      0.34       178\n",
      "           1       0.51      0.55      0.53       315\n",
      "           2       0.48      0.47      0.48       509\n",
      "           3       0.58      0.53      0.55       342\n",
      "           4       0.52      0.49      0.50       337\n",
      "           5       0.49      0.55      0.52       549\n",
      "\n",
      "    accuracy                           0.50      2230\n",
      "   macro avg       0.49      0.48      0.49      2230\n",
      "weighted avg       0.50      0.50      0.50      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.48834080717488787\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.40      0.35       178\n",
      "           1       0.45      0.57      0.50       315\n",
      "           2       0.54      0.38      0.44       509\n",
      "           3       0.52      0.55      0.53       342\n",
      "           4       0.54      0.53      0.54       337\n",
      "           5       0.51      0.51      0.51       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.48      0.49      0.48      2230\n",
      "weighted avg       0.50      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.452914798206278\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.34      0.29       178\n",
      "           1       0.48      0.46      0.47       315\n",
      "           2       0.45      0.45      0.45       509\n",
      "           3       0.52      0.54      0.53       342\n",
      "           4       0.48      0.51      0.49       337\n",
      "           5       0.46      0.40      0.43       549\n",
      "\n",
      "    accuracy                           0.45      2230\n",
      "   macro avg       0.44      0.45      0.44      2230\n",
      "weighted avg       0.46      0.45      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1300\n",
      "2    1199\n",
      "1     769\n",
      "3     761\n",
      "4     704\n",
      "0     470\n",
      "Name: Emotion, dtype: int64\n",
      "1    1300\n",
      "4    1300\n",
      "2    1300\n",
      "3    1300\n",
      "5    1300\n",
      "0    1300\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataiemocap['Phrase']\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectoriseurs √† tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LES N-GRAM POUR PLUS DE CONTEXTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNI ET BI GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.4385650224215247\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.38      0.24       178\n",
      "           1       0.54      0.46      0.50       315\n",
      "           2       0.47      0.36      0.41       509\n",
      "           3       0.57      0.46      0.51       342\n",
      "           4       0.55      0.31      0.40       337\n",
      "           5       0.44      0.58      0.50       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.46      0.43      0.43      2230\n",
      "weighted avg       0.48      0.44      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.4802690582959641\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.40      0.31       178\n",
      "           1       0.49      0.54      0.51       315\n",
      "           2       0.50      0.42      0.45       509\n",
      "           3       0.52      0.56      0.54       342\n",
      "           4       0.51      0.47      0.49       337\n",
      "           5       0.54      0.49      0.51       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.47      0.48      0.47      2230\n",
      "weighted avg       0.49      0.48      0.48      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.5201793721973094\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.29      0.29       178\n",
      "           1       0.56      0.52      0.54       315\n",
      "           2       0.56      0.45      0.50       509\n",
      "           3       0.60      0.55      0.57       342\n",
      "           4       0.60      0.55      0.58       337\n",
      "           5       0.47      0.62      0.54       549\n",
      "\n",
      "    accuracy                           0.52      2230\n",
      "   macro avg       0.51      0.50      0.50      2230\n",
      "weighted avg       0.53      0.52      0.52      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4964125560538117\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.40      0.32       178\n",
      "           1       0.57      0.57      0.57       315\n",
      "           2       0.49      0.49      0.49       509\n",
      "           3       0.55      0.52      0.54       342\n",
      "           4       0.59      0.47      0.52       337\n",
      "           5       0.50      0.50      0.50       549\n",
      "\n",
      "    accuracy                           0.50      2230\n",
      "   macro avg       0.49      0.49      0.49      2230\n",
      "weighted avg       0.51      0.50      0.50      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5466367713004484\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.31      0.36       178\n",
      "           1       0.61      0.53      0.57       315\n",
      "           2       0.48      0.57      0.52       509\n",
      "           3       0.70      0.52      0.60       342\n",
      "           4       0.71      0.50      0.58       337\n",
      "           5       0.49      0.66      0.56       549\n",
      "\n",
      "    accuracy                           0.55      2230\n",
      "   macro avg       0.57      0.51      0.53      2230\n",
      "weighted avg       0.57      0.55      0.55      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5201793721973094\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.35      0.39       178\n",
      "           1       0.54      0.56      0.55       315\n",
      "           2       0.49      0.48      0.48       509\n",
      "           3       0.64      0.51      0.57       342\n",
      "           4       0.52      0.52      0.52       337\n",
      "           5       0.50      0.60      0.55       549\n",
      "\n",
      "    accuracy                           0.52      2230\n",
      "   macro avg       0.52      0.50      0.51      2230\n",
      "weighted avg       0.52      0.52      0.52      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.5130044843049327\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.43      0.37       178\n",
      "           1       0.48      0.61      0.54       315\n",
      "           2       0.57      0.37      0.45       509\n",
      "           3       0.56      0.61      0.59       342\n",
      "           4       0.58      0.53      0.56       337\n",
      "           5       0.51      0.54      0.53       549\n",
      "\n",
      "    accuracy                           0.51      2230\n",
      "   macro avg       0.51      0.52      0.50      2230\n",
      "weighted avg       0.52      0.51      0.51      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.48161434977578477\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.42      0.32       178\n",
      "           1       0.59      0.45      0.51       315\n",
      "           2       0.49      0.41      0.45       509\n",
      "           3       0.54      0.53      0.54       342\n",
      "           4       0.55      0.45      0.49       337\n",
      "           5       0.47      0.57      0.52       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.48      0.47      0.47      2230\n",
      "weighted avg       0.50      0.48      0.48      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1300\n",
      "2    1199\n",
      "1     769\n",
      "3     761\n",
      "4     704\n",
      "0     470\n",
      "Name: Emotion, dtype: int64\n",
      "1    1300\n",
      "4    1300\n",
      "2    1300\n",
      "3    1300\n",
      "5    1300\n",
      "0    1300\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataiemocap['Phrase']\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# ------------------------Vectoriseurs √† tester + ON RAJOUTE LES NGRAM C'EST LA SEULE DIFFERENCE DANS CETTE CELLULE-------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(ngram_range=(1, 2)),\n",
    "    'TfidfVectorizer': TfidfVectorizer(ngram_range=(1, 2))\n",
    "}\n",
    "#~-----------------------------------------------------------------------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNI BI ET TRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.420627802690583\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.40      0.23       178\n",
      "           1       0.53      0.45      0.49       315\n",
      "           2       0.46      0.35      0.40       509\n",
      "           3       0.54      0.45      0.49       342\n",
      "           4       0.56      0.28      0.38       337\n",
      "           5       0.44      0.54      0.48       549\n",
      "\n",
      "    accuracy                           0.42      2230\n",
      "   macro avg       0.45      0.41      0.41      2230\n",
      "weighted avg       0.47      0.42      0.43      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.4896860986547085\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.39      0.32       178\n",
      "           1       0.50      0.55      0.52       315\n",
      "           2       0.52      0.42      0.47       509\n",
      "           3       0.52      0.56      0.54       342\n",
      "           4       0.50      0.47      0.48       337\n",
      "           5       0.53      0.52      0.52       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.48      0.49      0.48      2230\n",
      "weighted avg       0.50      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.520627802690583\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.30      0.31       178\n",
      "           1       0.57      0.49      0.53       315\n",
      "           2       0.57      0.44      0.49       509\n",
      "           3       0.60      0.54      0.57       342\n",
      "           4       0.61      0.57      0.59       337\n",
      "           5       0.46      0.64      0.53       549\n",
      "\n",
      "    accuracy                           0.52      2230\n",
      "   macro avg       0.52      0.50      0.50      2230\n",
      "weighted avg       0.53      0.52      0.52      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.47533632286995514\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.41      0.37       178\n",
      "           1       0.51      0.54      0.52       315\n",
      "           2       0.51      0.39      0.44       509\n",
      "           3       0.51      0.53      0.52       342\n",
      "           4       0.44      0.51      0.47       337\n",
      "           5       0.50      0.48      0.49       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.46      0.48      0.47      2230\n",
      "weighted avg       0.48      0.48      0.48      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5269058295964125\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.29      0.35       178\n",
      "           1       0.67      0.49      0.56       315\n",
      "           2       0.47      0.54      0.50       509\n",
      "           3       0.70      0.49      0.57       342\n",
      "           4       0.62      0.45      0.53       337\n",
      "           5       0.46      0.68      0.55       549\n",
      "\n",
      "    accuracy                           0.53      2230\n",
      "   macro avg       0.56      0.49      0.51      2230\n",
      "weighted avg       0.55      0.53      0.53      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5201793721973094\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.33      0.39       178\n",
      "           1       0.54      0.55      0.55       315\n",
      "           2       0.49      0.52      0.50       509\n",
      "           3       0.66      0.50      0.57       342\n",
      "           4       0.52      0.49      0.50       337\n",
      "           5       0.49      0.60      0.54       549\n",
      "\n",
      "    accuracy                           0.52      2230\n",
      "   macro avg       0.53      0.50      0.51      2230\n",
      "weighted avg       0.53      0.52      0.52      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.5147982062780269\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.44      0.37       178\n",
      "           1       0.49      0.61      0.54       315\n",
      "           2       0.58      0.38      0.46       509\n",
      "           3       0.56      0.63      0.59       342\n",
      "           4       0.59      0.52      0.55       337\n",
      "           5       0.51      0.54      0.52       549\n",
      "\n",
      "    accuracy                           0.51      2230\n",
      "   macro avg       0.51      0.52      0.51      2230\n",
      "weighted avg       0.53      0.51      0.51      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.47533632286995514\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.42      0.34       178\n",
      "           1       0.52      0.53      0.52       315\n",
      "           2       0.46      0.42      0.44       509\n",
      "           3       0.49      0.55      0.52       342\n",
      "           4       0.55      0.50      0.52       337\n",
      "           5       0.50      0.46      0.48       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.47      0.48      0.47      2230\n",
      "weighted avg       0.48      0.48      0.48      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1300\n",
      "2    1199\n",
      "1     769\n",
      "3     761\n",
      "4     704\n",
      "0     470\n",
      "Name: Emotion, dtype: int64\n",
      "1    1300\n",
      "4    1300\n",
      "2    1300\n",
      "3    1300\n",
      "5    1300\n",
      "0    1300\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = dataiemocap['Phrase']\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# ------------------------Vectoriseurs √† tester + ON RAJOUTE LES NGRAM C'EST LA SEULE DIFFERENCE DANS CETTE CELLULE-------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(ngram_range=(1, 3)),\n",
    "    'TfidfVectorizer': TfidfVectorizer(ngram_range=(1, 3))\n",
    "}\n",
    "#~-----------------------------------------------------------------------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENLEVER STOP WORDS ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.4031390134529148\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.37      0.23       178\n",
      "           1       0.46      0.41      0.43       315\n",
      "           2       0.46      0.36      0.41       509\n",
      "           3       0.53      0.46      0.49       342\n",
      "           4       0.47      0.34      0.40       337\n",
      "           5       0.40      0.45      0.42       549\n",
      "\n",
      "    accuracy                           0.40      2230\n",
      "   macro avg       0.41      0.40      0.40      2230\n",
      "weighted avg       0.43      0.40      0.41      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.442152466367713\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.44      0.29       178\n",
      "           1       0.45      0.47      0.46       315\n",
      "           2       0.47      0.41      0.44       509\n",
      "           3       0.54      0.54      0.54       342\n",
      "           4       0.43      0.43      0.43       337\n",
      "           5       0.52      0.41      0.46       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.44      0.45      0.44      2230\n",
      "weighted avg       0.46      0.44      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4461883408071749\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.40      0.28       178\n",
      "           1       0.46      0.50      0.48       315\n",
      "           2       0.48      0.41      0.44       509\n",
      "           3       0.52      0.48      0.50       342\n",
      "           4       0.54      0.51      0.52       337\n",
      "           5       0.46      0.41      0.43       549\n",
      "\n",
      "    accuracy                           0.45      2230\n",
      "   macro avg       0.45      0.45      0.44      2230\n",
      "weighted avg       0.47      0.45      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.42017937219730944\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.41      0.31       178\n",
      "           1       0.38      0.54      0.45       315\n",
      "           2       0.46      0.29      0.36       509\n",
      "           3       0.49      0.49      0.49       342\n",
      "           4       0.53      0.44      0.48       337\n",
      "           5       0.43      0.42      0.43       549\n",
      "\n",
      "    accuracy                           0.42      2230\n",
      "   macro avg       0.42      0.43      0.42      2230\n",
      "weighted avg       0.44      0.42      0.42      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.48878923766816146\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.26      0.35      0.30       178\n",
      "           1       0.56      0.44      0.49       315\n",
      "           2       0.46      0.53      0.49       509\n",
      "           3       0.67      0.49      0.56       342\n",
      "           4       0.57      0.51      0.54       337\n",
      "           5       0.47      0.51      0.49       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.50      0.47      0.48      2230\n",
      "weighted avg       0.51      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.46143497757847535\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.39      0.38       178\n",
      "           1       0.43      0.48      0.45       315\n",
      "           2       0.49      0.42      0.45       509\n",
      "           3       0.44      0.56      0.49       342\n",
      "           4       0.51      0.47      0.49       337\n",
      "           5       0.49      0.45      0.47       549\n",
      "\n",
      "    accuracy                           0.46      2230\n",
      "   macro avg       0.45      0.46      0.46      2230\n",
      "weighted avg       0.46      0.46      0.46      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4600896860986547\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.39      0.28       178\n",
      "           1       0.46      0.52      0.49       315\n",
      "           2       0.48      0.39      0.43       509\n",
      "           3       0.55      0.50      0.52       342\n",
      "           4       0.51      0.53      0.52       337\n",
      "           5       0.50      0.45      0.47       549\n",
      "\n",
      "    accuracy                           0.46      2230\n",
      "   macro avg       0.45      0.46      0.45      2230\n",
      "weighted avg       0.48      0.46      0.46      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4170403587443946\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.27      0.26       178\n",
      "           1       0.49      0.40      0.44       315\n",
      "           2       0.41      0.42      0.41       509\n",
      "           3       0.51      0.46      0.48       342\n",
      "           4       0.39      0.47      0.43       337\n",
      "           5       0.42      0.41      0.41       549\n",
      "\n",
      "    accuracy                           0.42      2230\n",
      "   macro avg       0.41      0.41      0.41      2230\n",
      "weighted avg       0.42      0.42      0.42      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1300\n",
      "2    1199\n",
      "1     769\n",
      "3     761\n",
      "4     704\n",
      "0     470\n",
      "Name: Emotion, dtype: int64\n",
      "1    1300\n",
      "4    1300\n",
      "2    1300\n",
      "3    1300\n",
      "5    1300\n",
      "0    1300\n",
      "Name: Emotion, dtype: int64\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ON VA UTILISER LES STOPSWORDS DE NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "X = dataiemocap['Phrase']\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "#-------------------- Vectoriseurs √† tester + ON PEUX METTRE LES STOPWORDS EN ARGUMENT DU VECTORISEUR ----------------------------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(stop_words=stop_words),\n",
    "    'TfidfVectorizer': TfidfVectorizer(stop_words=stop_words)\n",
    "}\n",
    "#--------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATISER ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.44349775784753365\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.22      0.38      0.28       178\n",
      "           1       0.54      0.52      0.53       315\n",
      "           2       0.49      0.38      0.43       509\n",
      "           3       0.50      0.46      0.48       342\n",
      "           4       0.46      0.36      0.40       337\n",
      "           5       0.44      0.52      0.48       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.44      0.44      0.43      2230\n",
      "weighted avg       0.46      0.44      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.4798206278026906\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.38      0.35       178\n",
      "           1       0.48      0.55      0.51       315\n",
      "           2       0.49      0.39      0.44       509\n",
      "           3       0.50      0.55      0.52       342\n",
      "           4       0.47      0.48      0.48       337\n",
      "           5       0.52      0.50      0.51       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.47      0.48      0.47      2230\n",
      "weighted avg       0.48      0.48      0.48      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.48475336322869955\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.35      0.31       178\n",
      "           1       0.48      0.51      0.49       315\n",
      "           2       0.52      0.42      0.47       509\n",
      "           3       0.52      0.52      0.52       342\n",
      "           4       0.54      0.53      0.53       337\n",
      "           5       0.49      0.52      0.50       549\n",
      "\n",
      "    accuracy                           0.48      2230\n",
      "   macro avg       0.47      0.48      0.47      2230\n",
      "weighted avg       0.49      0.48      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4538116591928251\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.38      0.34       178\n",
      "           1       0.50      0.42      0.46       315\n",
      "           2       0.45      0.40      0.42       509\n",
      "           3       0.50      0.52      0.51       342\n",
      "           4       0.46      0.46      0.46       337\n",
      "           5       0.45      0.50      0.48       549\n",
      "\n",
      "    accuracy                           0.45      2230\n",
      "   macro avg       0.45      0.45      0.44      2230\n",
      "weighted avg       0.46      0.45      0.45      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5493273542600897\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.36      0.38       178\n",
      "           1       0.60      0.57      0.58       315\n",
      "           2       0.50      0.59      0.54       509\n",
      "           3       0.67      0.53      0.59       342\n",
      "           4       0.67      0.50      0.58       337\n",
      "           5       0.51      0.61      0.55       549\n",
      "\n",
      "    accuracy                           0.55      2230\n",
      "   macro avg       0.56      0.53      0.54      2230\n",
      "weighted avg       0.56      0.55      0.55      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5125560538116591\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.37      0.35      0.36       178\n",
      "           1       0.51      0.56      0.53       315\n",
      "           2       0.50      0.48      0.49       509\n",
      "           3       0.62      0.53      0.57       342\n",
      "           4       0.52      0.51      0.52       337\n",
      "           5       0.51      0.57      0.54       549\n",
      "\n",
      "    accuracy                           0.51      2230\n",
      "   macro avg       0.51      0.50      0.50      2230\n",
      "weighted avg       0.51      0.51      0.51      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.49192825112107624\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.39      0.34       178\n",
      "           1       0.45      0.57      0.50       315\n",
      "           2       0.55      0.38      0.45       509\n",
      "           3       0.52      0.56      0.54       342\n",
      "           4       0.55      0.53      0.54       337\n",
      "           5       0.51      0.52      0.52       549\n",
      "\n",
      "    accuracy                           0.49      2230\n",
      "   macro avg       0.48      0.49      0.48      2230\n",
      "weighted avg       0.50      0.49      0.49      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4354260089686099\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.39      0.31       178\n",
      "           1       0.50      0.43      0.47       315\n",
      "           2       0.43      0.45      0.44       509\n",
      "           3       0.49      0.56      0.52       342\n",
      "           4       0.47      0.48      0.48       337\n",
      "           5       0.44      0.34      0.38       549\n",
      "\n",
      "    accuracy                           0.44      2230\n",
      "   macro avg       0.43      0.44      0.43      2230\n",
      "weighted avg       0.44      0.44      0.44      2230\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1300\n",
      "2    1199\n",
      "1     769\n",
      "3     761\n",
      "4     704\n",
      "0     470\n",
      "Name: Emotion, dtype: int64\n",
      "1    1300\n",
      "4    1300\n",
      "2    1300\n",
      "3    1300\n",
      "5    1300\n",
      "0    1300\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataiemocap['Phrase']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectoriseurs √† tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFIER TAILLE APPRENTISSAGE ET TAILLE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3 -> 0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.46402151983860124\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.39      0.27       114\n",
      "           1       0.55      0.51      0.53       218\n",
      "           2       0.52      0.40      0.45       341\n",
      "           3       0.60      0.51      0.55       240\n",
      "           4       0.56      0.42      0.48       227\n",
      "           5       0.41      0.52      0.46       347\n",
      "\n",
      "    accuracy                           0.46      1487\n",
      "   macro avg       0.48      0.46      0.46      1487\n",
      "weighted avg       0.50      0.46      0.47      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5090786819098857\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.44      0.38       114\n",
      "           1       0.56      0.65      0.60       218\n",
      "           2       0.52      0.44      0.48       341\n",
      "           3       0.53      0.56      0.54       240\n",
      "           4       0.50      0.49      0.50       227\n",
      "           5       0.53      0.49      0.51       347\n",
      "\n",
      "    accuracy                           0.51      1487\n",
      "   macro avg       0.50      0.51      0.50      1487\n",
      "weighted avg       0.51      0.51      0.51      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.49159381304640215\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.33      0.27       114\n",
      "           1       0.51      0.54      0.52       218\n",
      "           2       0.51      0.40      0.45       341\n",
      "           3       0.58      0.53      0.55       240\n",
      "           4       0.58      0.53      0.55       227\n",
      "           5       0.49      0.55      0.52       347\n",
      "\n",
      "    accuracy                           0.49      1487\n",
      "   macro avg       0.48      0.48      0.48      1487\n",
      "weighted avg       0.50      0.49      0.50      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4714189643577673\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.32      0.29       114\n",
      "           1       0.49      0.54      0.51       218\n",
      "           2       0.47      0.44      0.45       341\n",
      "           3       0.59      0.50      0.54       240\n",
      "           4       0.48      0.53      0.50       227\n",
      "           5       0.47      0.45      0.46       347\n",
      "\n",
      "    accuracy                           0.47      1487\n",
      "   macro avg       0.46      0.46      0.46      1487\n",
      "weighted avg       0.48      0.47      0.47      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5736381977135171\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.37      0.41       114\n",
      "           1       0.61      0.59      0.60       218\n",
      "           2       0.51      0.59      0.55       341\n",
      "           3       0.73      0.57      0.64       240\n",
      "           4       0.68      0.56      0.62       227\n",
      "           5       0.51      0.62      0.56       347\n",
      "\n",
      "    accuracy                           0.57      1487\n",
      "   macro avg       0.59      0.55      0.56      1487\n",
      "weighted avg       0.59      0.57      0.57      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5339609952925353\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.43      0.41       114\n",
      "           1       0.57      0.60      0.58       218\n",
      "           2       0.50      0.47      0.49       341\n",
      "           3       0.67      0.57      0.62       240\n",
      "           4       0.57      0.52      0.55       227\n",
      "           5       0.49      0.57      0.53       347\n",
      "\n",
      "    accuracy                           0.53      1487\n",
      "   macro avg       0.53      0.53      0.53      1487\n",
      "weighted avg       0.54      0.53      0.53      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.4996637525218561\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.31      0.46      0.37       114\n",
      "           1       0.50      0.61      0.55       218\n",
      "           2       0.56      0.39      0.46       341\n",
      "           3       0.55      0.55      0.55       240\n",
      "           4       0.54      0.49      0.51       227\n",
      "           5       0.49      0.52      0.50       347\n",
      "\n",
      "    accuracy                           0.50      1487\n",
      "   macro avg       0.49      0.50      0.49      1487\n",
      "weighted avg       0.51      0.50      0.50      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4566240753194351\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.44      0.36       114\n",
      "           1       0.51      0.52      0.51       218\n",
      "           2       0.46      0.38      0.42       341\n",
      "           3       0.50      0.61      0.55       240\n",
      "           4       0.49      0.46      0.48       227\n",
      "           5       0.43      0.38      0.41       347\n",
      "\n",
      "    accuracy                           0.46      1487\n",
      "   macro avg       0.45      0.47      0.45      1487\n",
      "weighted avg       0.46      0.46      0.46      1487\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1502\n",
      "2    1367\n",
      "1     866\n",
      "3     863\n",
      "4     814\n",
      "0     534\n",
      "Name: Emotion, dtype: int64\n",
      "2    1502\n",
      "3    1502\n",
      "5    1502\n",
      "1    1502\n",
      "4    1502\n",
      "0    1502\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataiemocap['Phrase']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Vectoriseurs √† tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2 -> 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVM with CountVectorizer...\n",
      "-----------------SVM with CountVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.4320780094149294\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.41      0.29       254\n",
      "           1       0.47      0.50      0.48       412\n",
      "           2       0.46      0.36      0.40       689\n",
      "           3       0.48      0.48      0.48       450\n",
      "           4       0.52      0.34      0.41       428\n",
      "           5       0.44      0.50      0.47       741\n",
      "\n",
      "    accuracy                           0.43      2974\n",
      "   macro avg       0.44      0.43      0.42      2974\n",
      "weighted avg       0.45      0.43      0.43      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with CountVectorizer...\n",
      "-----------------Random Forest with CountVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.4734364492266308\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.38      0.34       254\n",
      "           1       0.46      0.59      0.51       412\n",
      "           2       0.48      0.42      0.45       689\n",
      "           3       0.48      0.56      0.52       450\n",
      "           4       0.51      0.42      0.46       428\n",
      "           5       0.53      0.47      0.50       741\n",
      "\n",
      "    accuracy                           0.47      2974\n",
      "   macro avg       0.46      0.47      0.46      2974\n",
      "weighted avg       0.48      0.47      0.47      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with CountVectorizer...\n",
      "-----------------Multinomial Naive Bayes with CountVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.47209145931405516\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.37      0.33       254\n",
      "           1       0.46      0.51      0.48       412\n",
      "           2       0.52      0.41      0.46       689\n",
      "           3       0.49      0.51      0.50       450\n",
      "           4       0.51      0.50      0.51       428\n",
      "           5       0.49      0.50      0.49       741\n",
      "\n",
      "    accuracy                           0.47      2974\n",
      "   macro avg       0.46      0.47      0.46      2974\n",
      "weighted avg       0.48      0.47      0.47      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with CountVectorizer...\n",
      "-----------------Perceptron with CountVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4468728984532616\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.39      0.33       254\n",
      "           1       0.51      0.46      0.48       412\n",
      "           2       0.45      0.34      0.39       689\n",
      "           3       0.46      0.61      0.52       450\n",
      "           4       0.49      0.49      0.49       428\n",
      "           5       0.46      0.43      0.44       741\n",
      "\n",
      "    accuracy                           0.45      2974\n",
      "   macro avg       0.44      0.45      0.44      2974\n",
      "weighted avg       0.45      0.45      0.45      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training SVM with TfidfVectorizer...\n",
      "-----------------SVM with TfidfVectorizer------------------\n",
      "\n",
      "Results for SVM:\n",
      "Accuracy: 0.5332885003362475\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.39      0.43       254\n",
      "           1       0.56      0.56      0.56       412\n",
      "           2       0.48      0.55      0.51       689\n",
      "           3       0.64      0.53      0.58       450\n",
      "           4       0.64      0.50      0.56       428\n",
      "           5       0.49      0.57      0.53       741\n",
      "\n",
      "    accuracy                           0.53      2974\n",
      "   macro avg       0.55      0.52      0.53      2974\n",
      "weighted avg       0.54      0.53      0.53      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Random Forest with TfidfVectorizer...\n",
      "-----------------Random Forest with TfidfVectorizer------------------\n",
      "\n",
      "Results for Random Forest:\n",
      "Accuracy: 0.5060524546065904\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.36      0.40       254\n",
      "           1       0.50      0.56      0.53       412\n",
      "           2       0.51      0.47      0.49       689\n",
      "           3       0.58      0.53      0.55       450\n",
      "           4       0.50      0.52      0.51       428\n",
      "           5       0.50      0.54      0.52       741\n",
      "\n",
      "    accuracy                           0.51      2974\n",
      "   macro avg       0.50      0.50      0.50      2974\n",
      "weighted avg       0.51      0.51      0.51      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Multinomial Naive Bayes with TfidfVectorizer...\n",
      "-----------------Multinomial Naive Bayes with TfidfVectorizer------------------\n",
      "\n",
      "Results for Multinomial Naive Bayes:\n",
      "Accuracy: 0.47982515131136516\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.34      0.43      0.38       254\n",
      "           1       0.43      0.61      0.51       412\n",
      "           2       0.54      0.35      0.42       689\n",
      "           3       0.49      0.58      0.53       450\n",
      "           4       0.56      0.48      0.51       428\n",
      "           5       0.50      0.49      0.49       741\n",
      "\n",
      "    accuracy                           0.48      2974\n",
      "   macro avg       0.48      0.49      0.47      2974\n",
      "weighted avg       0.49      0.48      0.48      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "Training Perceptron with TfidfVectorizer...\n",
      "-----------------Perceptron with TfidfVectorizer------------------\n",
      "\n",
      "Results for Perceptron:\n",
      "Accuracy: 0.4344317417619368\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.29      0.37      0.32       254\n",
      "           1       0.49      0.49      0.49       412\n",
      "           2       0.41      0.37      0.39       689\n",
      "           3       0.48      0.55      0.52       450\n",
      "           4       0.43      0.50      0.46       428\n",
      "           5       0.47      0.38      0.42       741\n",
      "\n",
      "    accuracy                           0.43      2974\n",
      "   macro avg       0.43      0.44      0.43      2974\n",
      "weighted avg       0.44      0.43      0.43      2974\n",
      "\n",
      "--------------------------------------------------\n",
      "5    1108\n",
      "2    1019\n",
      "1     672\n",
      "3     653\n",
      "4     613\n",
      "0     394\n",
      "Name: Emotion, dtype: int64\n",
      "0    1108\n",
      "3    1108\n",
      "2    1108\n",
      "4    1108\n",
      "5    1108\n",
      "1    1108\n",
      "Name: Emotion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = dataiemocap['Phrase']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n",
    "# Vectoriseurs √† tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Movie datasets\n",
    "dataiemocap = preprocess_comma(\"output.csv\",\"dataiemocap.csv\")\n",
    "#REDUCTION DU DATAFRAME CAR TROP D'INSTANCES \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = dataiemocap['Phrase']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "y = dataiemocap['Emotion']\n",
    "\n",
    "# Diviser les donn√©es en ensembles d'entra√Ænement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Vectoriseurs √† tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le sur√©chantillonnage al√©atoire apr√®s la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Cr√©er le pipeline avec le mod√®le choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entra√Æner le mod√®le sur les donn√©es resampl√©es\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Pr√©dictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # √âvaluation des performances du mod√®le\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "        \n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())      \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
