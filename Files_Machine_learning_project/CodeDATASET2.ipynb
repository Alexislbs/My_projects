{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\incen\\anaconda3\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.9.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\incen\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2120\\2922955518.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#SKLEARN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\incen\\anaconda3\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcluster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload_shell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\incen\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   2473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m \u001b[1;31m# Aliases\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2475\u001b[1;33m \u001b[0m_downloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDownloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2476\u001b[0m \u001b[0mdownload\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\incen\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m         \u001b[1;31m# decide where we're going to save things to.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_download_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m     \u001b[1;31m# /////////////////////////////////////////////////////////////////\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\incen\\anaconda3\\lib\\site-packages\\nltk\\downloader.py\u001b[0m in \u001b[0;36mdefault_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[1;31m# Check if we have sufficient permissions to install in a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;31m# variety of system-wide locations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mnltkdir\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minternals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_writable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltkdir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mnltkdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "#IMPORTATIONS GENERALES\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "#SKLEARN \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "   #LES 4 MODELES\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "#AUTRE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# NLTK\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FONCTIONS ET DICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_comma(file,newfile):\n",
    "    data_frame = pd.read_csv(file)\n",
    "\n",
    "    # Retirer les virgules sauf la première de chaque ligne\n",
    "    data_frame.iloc[:, 7:] =  data_frame.iloc[:, 7:].replace(to_replace=',', value='', regex=True)\n",
    "# Enregistrer dans un nouveau fichier CSV avec un autre nom\n",
    "    nom_fichier_sortie = newfile\n",
    "    data_frame.to_csv(nom_fichier_sortie, index=False)\n",
    "    return data_frame\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "def regrouper_genres(genre):\n",
    "    genre = genre.lower()  # Convertir en minuscules pour une correspondance insensible à la casse\n",
    "    \n",
    "    if any(keyword in genre for keyword in ['comedy', 'romance']):\n",
    "        return 'Comedy/Romance'\n",
    "    elif any(keyword in genre for keyword in ['action', 'thriller', 'suspense']):\n",
    "        return 'Action/Thriller'\n",
    "    elif any(keyword in genre for keyword in ['drama', 'melodrama']):\n",
    "        return 'Drama/Melodrama'\n",
    "    elif any(keyword in genre for keyword in ['sci-fi', 'science fiction', 'fantasy']):\n",
    "        return 'Sci-Fi/Fantasy'\n",
    "    elif 'horror' in genre:\n",
    "        return 'Horror'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "models = {\n",
    "    'SVM': SVC(class_weight='balanced'),\n",
    "    'Random Forest': RandomForestClassifier(class_weight='balanced'),\n",
    "    'Multinomial Naive Bayes': MultinomialNB(),\n",
    "    'Perceptron': Perceptron(class_weight='balanced')\n",
    "}\n",
    "\n",
    "def printresult(model_name,accuracy,classification_report_result):\n",
    "    print(f'\\nResults for {model_name}:')\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "    print('Classification Report:')\n",
    "    print(classification_report_result)\n",
    "    print('-' * 50)\n",
    "\n",
    "\n",
    "def evaluation(predictions,y_test):\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    classification_report_result = classification_report(y_test, predictions)\n",
    "    return accuracy, classification_report_result\n",
    "\n",
    "def lemmatiser(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREMIERE MISE EN FORME DES DONNEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Movie datasets\n",
    "datamovie = preprocess_comma(\"movieplot.csv\",\"datamovie.csv\")\n",
    "#REDUCTION DU DATAFRAME CAR TROP D'INSTANCES \n",
    "\n",
    "#ON CHOISI LE NOMBRE D INSTANCES\n",
    "datamovie= datamovie.sample(n=1000, random_state=7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Appliquer la fonction sur la colonne 'Genre' pour créer une nouvelle colonne 'Genre Regroupe'\n",
    "datamovie['Genre'] = datamovie['Genre'].apply(regrouper_genres)\n",
    "#TROP DE CLASSES DONC ON ENLEVE OTHER\n",
    "datamovie = datamovie[datamovie['Genre'] != 'Other']\n",
    "# Réinitialiser les index après la suppression des lignes\n",
    "datamovie.reset_index(drop=True, inplace=True)\n",
    "\n",
    "X = datamovie['Plot']\n",
    "y = datamovie['Genre']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    pipeline1 = make_pipeline(CountVectorizer(), model)\n",
    "    pipeline2 = make_pipeline(TfidfVectorizer(), model)\n",
    "# Entraîner le modèle\n",
    "    pipeline1.fit(X_train, y_train)\n",
    "    pipeline2.fit(X_train, y_train)\n",
    "# Prédictions sur l'ensemble de test\n",
    "    predictions1 = pipeline1.predict(X_test)\n",
    "    predictions2 = pipeline2.predict(X_test)\n",
    "    \n",
    "# Évaluation des performances du modèle avec COUNTVECTORIZER \n",
    "    accuracy1 , classification_report_result1 = evaluation(predictions1,y_test)\n",
    "    accuracy2 , classification_report_result2 = evaluation(predictions2,y_test)\n",
    "    # Affichage\n",
    "    print(\"-----------------COUNTVECTORIZER------------------\")\n",
    "    printresult(model_name,accuracy1,classification_report_result1)\n",
    "    print(\"---------------------TFIDF------------------------\")\n",
    "    printresult(model_name,accuracy2,classification_report_result2)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ON VA SURECHANTILLONNER LES CLASSES SOUS REPRESENTEES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datamovie['Plot']\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LES N-GRAM POUR PLUS DE CONTEXTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNI ET BI GRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datamovie['Plot']\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# ------------------------Vectoriseurs à tester + ON RAJOUTE LES NGRAM C'EST LA SEULE DIFFERENCE DANS CETTE CELLULE-------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(ngram_range=(1, 2)),\n",
    "    'TfidfVectorizer': TfidfVectorizer(ngram_range=(1, 2))\n",
    "}\n",
    "#~-----------------------------------------------------------------------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNI BI ET TRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = datamovie['Plot']\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# ------------------------Vectoriseurs à tester + ON RAJOUTE LES NGRAM C'EST LA SEULE DIFFERENCE DANS CETTE CELLULE-------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(ngram_range=(1, 3)),\n",
    "    'TfidfVectorizer': TfidfVectorizer(ngram_range=(1, 3))\n",
    "}\n",
    "#~-----------------------------------------------------------------------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENLEVER STOP WORDS ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ON VA UTILISER LES STOPSWORDS DE NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "X = datamovie['Plot']\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "#-------------------- Vectoriseurs à tester + ON PEUX METTRE LES STOPWORDS EN ARGUMENT DU VECTORISEUR ----------------------------------\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(stop_words=stop_words),\n",
    "    'TfidfVectorizer': TfidfVectorizer(stop_words=stop_words)\n",
    "}\n",
    "#--------------------------------------------------------------\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMMATISER ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = datamovie['Plot']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFIER TAILLE APPRENTISSAGE ET TAILLE TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3 -> 0,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = datamovie['Plot']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2 -> 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = datamovie['Plot']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUGMENTER LES DONNEES ? 1000 -> 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Movie datasets\n",
    "datamovie = preprocess_comma(\"movieplot.csv\",\"datamovie.csv\")\n",
    "#REDUCTION DU DATAFRAME CAR TROP D'INSTANCES \n",
    "\n",
    "#ON CHOISI LE NOMBRE D INSTANCES ici 4000\n",
    "datamovie= datamovie.sample(n=4000, random_state=7)\n",
    "# Appliquer la fonction sur la colonne 'Genre' pour créer une nouvelle colonne 'Genre Regroupe'\n",
    "datamovie['Genre'] = datamovie['Genre'].apply(regrouper_genres)\n",
    "#TROP DE CLASSES DONC ON ENLEVE OTHER\n",
    "datamovie = datamovie[datamovie['Genre'] != 'Other']\n",
    "# Réinitialiser les index après la suppression des lignes\n",
    "datamovie.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = datamovie['Plot']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())      \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1000 -> 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#Movie datasets\n",
    "datamovie = preprocess_comma(\"movieplot.csv\",\"datamovie.csv\")\n",
    "#REDUCTION DU DATAFRAME CAR TROP D'INSTANCES \n",
    "\n",
    "#ON CHOISI LE NOMBRE D INSTANCES ici 10000\n",
    "datamovie= datamovie.sample(n=10000, random_state=7)\n",
    "# Appliquer la fonction sur la colonne 'Genre' pour créer une nouvelle colonne 'Genre Regroupe'\n",
    "datamovie['Genre'] = datamovie['Genre'].apply(regrouper_genres)\n",
    "#TROP DE CLASSES DONC ON ENLEVE OTHER\n",
    "datamovie = datamovie[datamovie['Genre'] != 'Other']\n",
    "# Réinitialiser les index après la suppression des lignes\n",
    "datamovie.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X = datamovie['Plot']\n",
    "#---------------------C'EST ICI QU'ON LEMMATISE AVEC WORD NET ---------------------------\n",
    "X = X.apply(lemmatiser)\n",
    "#----------------------------------------------------------------------------------------\n",
    "y = datamovie['Genre']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# Vectoriseurs à tester\n",
    "vectorizers = {\n",
    "    'CountVectorizer': CountVectorizer(),\n",
    "    'TfidfVectorizer': TfidfVectorizer()\n",
    "}\n",
    "\n",
    "for vectorizer_name, vectorizer in vectorizers.items():\n",
    "    # Appliquer le suréchantillonnage aléatoire après la vectorisation\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = ros.fit_resample(X_train_vectorized, y_train)\n",
    "   \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} with {vectorizer_name}...\")\n",
    "        \n",
    "        # Créer le pipeline avec le modèle choisi\n",
    "        model_pipeline = make_pipeline(model)\n",
    "        \n",
    "        # Entraîner le modèle sur les données resamplées\n",
    "        model_pipeline.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "        # Prédictions sur l'ensemble de test\n",
    "        predictions = model_pipeline.predict(vectorizer.transform(X_test))\n",
    "        \n",
    "        # Évaluation des performances du modèle\n",
    "        accuracy, classification_report_result = evaluation(predictions, y_test)\n",
    "        \n",
    "        # Affichage\n",
    "        print(f\"-----------------{model_name} with {vectorizer_name}------------------\")\n",
    "        printresult(model_name, accuracy, classification_report_result)\n",
    "        \n",
    "print(y_train.value_counts())\n",
    "print(y_train_resampled.value_counts())      \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
